
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>API Reference &#8212; mptorch 0.3.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=e259d695"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Examples" href="examples.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">mptorch 0.3.0 documentation</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="usage.html">
    Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorial.html">
    Tutorial
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="examples.html">
    Examples
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/mptorch/mptorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github-square fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="usage.html">
    Usage
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tutorial.html">
    Tutorial
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="examples.html">
    Examples
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/mptorch/mptorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fab fa-github-square fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">API Reference</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="api-reference">
<span id="id1"></span><h1>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h1>
<section id="module-mptorch.number">
<span id="number-formats"></span><span id="api-number"></span><h2>Number formats<a class="headerlink" href="#module-mptorch.number" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.Number">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">Number</span></span><a class="reference internal" href="_modules/mptorch/number.html#Number"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.Number" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Base class for all supported number formats.</p>
<p>Users should always instantiate one of the derived classes.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.FloatType">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">FloatType</span></span><a class="reference internal" href="_modules/mptorch/number.html#FloatType"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.FloatType" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a></p>
<p>Base class for all float-like number formats.</p>
<p>Similar to the <a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> class, users should not instantiate
this class directly. It is useful as a means to determine if
a number format is of float type.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.FixedPoint">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">FixedPoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clamp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/number.html#FixedPoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.FixedPoint" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a></p>
<p>Low-Precision Fixed Point Number Format. Defined similarly to
<em>Deep Learning with Limited Numerical Precision</em> (<a class="reference external" href="https://arxiv.org/abs/1502.02551">https://arxiv.org/abs/1502.02551</a>).</p>
<p>The representable range is <span class="math notranslate nohighlight">\(\left[-2^{\text{wl}-\text{fl}-1},
2^{\text{wl}-\text{fl}-1}-2^{-\text{fl}}\right]\)</span>
and the precision unit (smallest nonzero absolute value) is <span class="math notranslate nohighlight">\(2^{-\text{fl}}\)</span>.
Numbers outside of the representable range can be clamped (if <cite>clamp</cite> is true).
We can also give up the smallest representable number to make the range symmetric,
<span class="math notranslate nohighlight">\(\left[-2^{\text{wl}-\text{fl}-1}+2^{-\text{fl}},
2^{\textnormal{wl}-\text{fl}-1}-2^{-\text{fl}}\right]\)</span> (if <cite>symmetric</cite> is true).</p>
<p>Define <span class="math notranslate nohighlight">\(\lfloor x \rfloor\)</span> to be the largest representable number (multiples of <span class="math notranslate nohighlight">\(2^-\text{fl}\)</span>)
smaller than <span class="math notranslate nohighlight">\(x\)</span>. For numbers within the representable range, we support two kinds of fixed point
quantization: <em>round to nearest</em> (RN) and <em>stochastic rounding</em> (SR). They correspond to</p>
<div class="math notranslate nohighlight">
\[\text{RN}(x)
=
\Biggl \lbrace
{
\lfloor x \rfloor, \text{ if } \lfloor x \rfloor \leq x \leq \lfloor x \rfloor + 2^{-\text{fl}-1}
\atop
 \lfloor x \rfloor + 2^{-\text{fl}}, \text{ if } \lfloor x \rfloor + 2^{-\text{fl}-1} &lt; x \leq \lfloor x \rfloor + 2^{-\text{fl}}
}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\textnormal{SR}(x)
=
\Biggl \lbrace
{
\lfloor x \rfloor, \text{ with probabilty } 1 - \frac{x - \lfloor x \rfloor}{2^{-\text{fl}}}
\atop
 \lfloor x \rfloor + 2^{-\text{fl}}, \text{ with probabilty } \frac{x - \lfloor x \rfloor}{2^{-\text{fl}}}
}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>wl</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – word length of each fixed point number</p></li>
<li><p><strong>fl</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – fractional length of each fixed point number</p></li>
<li><p><strong>clamp</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether to clamp unrepresentable numbers</p></li>
<li><p><strong>symmetric</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether to make the representable range symmetric</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.FloatingPoint">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">FloatingPoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">man</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subnormals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saturate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/number.html#FloatingPoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.FloatingPoint" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mptorch.number.FloatType" title="mptorch.number.FloatType"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code></a></p>
<p>Low-Precision Floating Point Format. Follows rules set out in the IEEE-754 standard, applying
them in the context of custom precision formats.</p>
<p>We set the exponent bias to be <span class="math notranslate nohighlight">\(2^{\text{exp}-1} - 1\)</span>. In terms of rounding mode (see
available quantization functions), we offer support for <em>round to nearest even</em> and <em>stochastic rounding</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>exp</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for exponent</p></li>
<li><p><strong>man</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for mantissa, referring to number of bits that are
supposed to be stored on hardware (not counting the virtual bits)</p></li>
<li><p><strong>subnormals</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – allow the use of subnormal values</p></li>
<li><p><strong>saturate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – clamp values instead of using infinities in case of overflow</p></li>
</ul>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="mptorch.number.FloatingPoint.is_fp32">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_fp32</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#mptorch.number.FloatingPoint.is_fp32" title="Link to this definition">#</a></dt>
<dd><p>Returns if the format is equivalent to the IEEE-754 <code class="docutils literal notranslate"><span class="pre">binary32</span></code> format.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mptorch.number.FloatingPoint.is_fp16">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_fp16</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#mptorch.number.FloatingPoint.is_fp16" title="Link to this definition">#</a></dt>
<dd><p>Returns if the format is equivalent to the IEEE-754 <code class="docutils literal notranslate"><span class="pre">binary16</span></code> format.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="mptorch.number.FloatingPoint.is_bfloat16">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_bfloat16</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#mptorch.number.FloatingPoint.is_bfloat16" title="Link to this definition">#</a></dt>
<dd><p>Returns if the format is equivalent to the <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> format.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.BlockFloatingPoint">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">BlockFloatingPoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/number.html#BlockFloatingPoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.BlockFloatingPoint" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a></p>
<p>Low-Precision Block Floating Point Format.</p>
<p>BlockFloatingPoint shares an exponent across a block of numbers. The shared
exponent is chosen from the largest magnitude in the block.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>wl</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – word length of the tensor</p></li>
<li><p><strong>dim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – block dimension to share exponent. (*, D, *) Tensor where
D is at position <cite>dim</cite> will have D different exponents; use -1 if the
entire tensor is treated as a single block (there is only 1 shared
exponent).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.SuperNormalFloat">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">SuperNormalFloat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">exp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">man</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binades</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saturate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/number.html#SuperNormalFloat"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.SuperNormalFloat" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mptorch.number.FloatType" title="mptorch.number.FloatType"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code></a></p>
<p>Low-Precision SuperNormal Floating Point Format. Described in
<em>Range Extension with Supernormals for Mixed-Precision 8-bit DNN Training</em>
(<a class="reference external" href="https://www.arith2025.org/proceedings/215900a017.pdf">https://www.arith2025.org/proceedings/215900a017.pdf</a>)</p>
<p>We set the exponent bias to be <span class="math notranslate nohighlight">\(2^{\text{exp}-1}\)</span>. For rounding
mode, we apply <em>round to nearest even</em>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>exp</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for exponent</p></li>
<li><p><strong>man</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for mantissa, referring to number of bits that are
supposed to be stored on hardware (not counting the virtual bits)</p></li>
<li><p><strong>binades</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – number of binades transformed into log range</p></li>
<li><p><strong>saturate</strong> – clamp values instead of using infinities in case of overflow</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.number.Binary8">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.number.</span></span><span class="sig-name descname"><span class="pre">Binary8</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">signed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subnormals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overflow_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'saturate_maxfloat2'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/number.html#Binary8"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.number.Binary8" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#mptorch.number.FloatType" title="mptorch.number.FloatType"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code></a></p>
<p>Low-Precision <code class="docutils literal notranslate"><span class="pre">binary8</span></code> Format the follows the IEEE-P3109 WG specification (<a class="github reference external" href="https://github.com/P3109/Public/">P3109/Public</a>).
This is a format showcasing an evolving standard. Changes are likely in the future.</p>
<p><code class="docutils literal notranslate"><span class="pre">binary8</span></code> is a format that takes a value P as an input to determines the number
of mantissa and exponent bits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>P</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – integer precision of the <code class="docutils literal notranslate"><span class="pre">binary8</span></code> format</p></li>
<li><p><strong>signed</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – boolean indicating whether the format is signed or unsigned</p></li>
<li><p><strong>subnormals</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – allow the use of subnormal values</p></li>
<li><p><strong>overflow_policy</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'saturarte_infty'</span></code>, <code class="docutils literal notranslate"><span class="pre">'saturate_maxfloat'</span></code>, <code class="docutils literal notranslate"><span class="pre">'saturate_maxfloat2'</span></code>]</span>) – string indicating the overflow policy, one of:
<code class="docutils literal notranslate"><span class="pre">saturate_maxfloat2</span></code> (no infinity and +1 normalized value),
<code class="docutils literal notranslate"><span class="pre">saturate_maxfloat</span></code> (clamp to maxfloat),
<code class="docutils literal notranslate"><span class="pre">saturate_infty</span></code> (use infinity)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-mptorch.quant">
<span id="quantization"></span><span id="api-quant"></span><h2>Quantization<a class="headerlink" href="#module-mptorch.quant" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.fixed_point_quantize">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">fixed_point_quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clamp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'stochastic'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_function.html#fixed_point_quantize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.fixed_point_quantize" title="Link to this definition">#</a></dt>
<dd><p>Quantize a single precision floating-point tensor into a low-precision fixed-point tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the single precision tensor to be quantized</p></li>
<li><p><strong>wl</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – word length of the fixed-point format being simulated</p></li>
<li><p><strong>fl</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – fractional length of the fixed-point format being simulated</p></li>
<li><p><strong>clamp</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – clamp input numbers into representable range. If false, the quantization will only simulate the effect on precision</p></li>
<li><p><strong>symmetric</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – discard the minimum representable number to make the representable range symmetric</p></li>
<li><p><strong>rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, “stochastic” or “nearest” (default: “stochastic”)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a quantized fixed-point representation of the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.block_quantize">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">block_quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'stochastic'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_function.html#block_quantize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.block_quantize" title="Link to this definition">#</a></dt>
<dd><p>Quantize a single precision floating-point tensor into a low-precision block floating-point representation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the single precision tensor to be quantized</p></li>
<li><p><strong>wl</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – word length of the block floating-point format being simulated</p></li>
<li><p><strong>dim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – dimension over which to apply the block floating point representation (-1 applies it to the entire tensor)</p></li>
<li><p><strong>rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, “stochastic” or “nearest”</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a quantized low-precision block floating-point representation of the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.float_quantize">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">float_quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">man</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'stochastic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subnormals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saturate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prng</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_function.html#float_quantize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.float_quantize" title="Link to this definition">#</a></dt>
<dd><p>Quantize a single precision floating-point tensor into a IEEE-754-like low-precision floating-point tensor</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the single precision number to be quantized</p></li>
<li><p><strong>exp</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for exponent</p></li>
<li><p><strong>man</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for mantissa, not counting the virtual bit</p></li>
<li><p><strong>rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, “stochastic” or “nearest”</p></li>
<li><p><strong>subnormals</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – if subnormals are supported or not</p></li>
<li><p><strong>saturate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – saturate on overflow or use infinities</p></li>
<li><p><strong>prng</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of random bits to use in case of stochastic rounding</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a quantized low-precision floating point representation of the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.binary8_quantize">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">binary8_quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">P</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overflow_policy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'saturate_maxfloat'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_signed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subnormals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prng_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_function.html#binary8_quantize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.binary8_quantize" title="Link to this definition">#</a></dt>
<dd><p>Quantize a single precision floating-point tensor into a P3109-compatible one</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the single precision number(torch.Tensor) to be quantized</p></li>
<li><p><strong>P</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for precision</p></li>
<li><p><strong>is_signed</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – if subnormals are supported or not</p></li>
<li><p><strong>rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>, <code class="docutils literal notranslate"><span class="pre">'truncate'</span></code>]</span>) – the quantization rounding mode</p></li>
<li><p><strong>overflow_policy</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'saturate_infty'</span></code>, <code class="docutils literal notranslate"><span class="pre">'saturate_maxfloat'</span></code>, <code class="docutils literal notranslate"><span class="pre">'saturate_maxfloat2'</span></code>]</span>) – overflow handling policy</p></li>
<li><p><strong>subnormals</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – saturate on overflow or use infinities</p></li>
<li><p><strong>prng_bits</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits for the random generator</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Overflow Policies:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">saturate_infty</span></code>: Finite input values of binary32, exceeding the maximum float value of the binary8 format, will saturate to the maximum float. Infinite inputs will still map to infinities in this mode.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">saturate_maxfloat</span></code>: Both finite and infinite input values of binary32, exceeding the maximum float value of the binary8 format, will saturate to the maximum float represented by 0x7e/0xfe. This number system has an encoding reserved for infinity (0x7f/0xff).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">saturate_maxfloat2</span></code>: Both finite and infinite input values of binary32, exceeding the maximum float value of the binary8 format, will saturate to the maximum float represented by 0x7f/0xff. This number system does not have an encoding reserved for infinity.</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a quantized low-precision floating point representation of the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.superfp_quantize">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">superfp_quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exp</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">man</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binades</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saturate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_function.html#superfp_quantize"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.superfp_quantize" title="Link to this definition">#</a></dt>
<dd><p>Quantize a single precision floating-point tensor into a low-precision supernormal floating-point one</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the single precision number to be quantized</p></li>
<li><p><strong>exp</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for exponent</p></li>
<li><p><strong>man</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of bits allocated for mantissa, not counting the virtual bit</p></li>
<li><p><strong>binades</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – number of binades that will be transformed into log range</p></li>
<li><p><strong>rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>, <code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>]</span>) – rounding mode, “stochastic” or “nearest”</p></li>
<li><p><strong>saturate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – saturate on overflow or use infinities</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a quantized low-precision supernormal floating-point representation of the input tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.quantizer">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">quantizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_number</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_number</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'stochastic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'stochastic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clamping_grad_zero</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_function.html#quantizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.quantizer" title="Link to this definition">#</a></dt>
<dd><p>Creates a quantization function to support quantizing forward and backward process differently.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_number</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the number format used for forward quantization.
if is None, the quantization would be a identity mapping.</p></li>
<li><p><strong>backward_number</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the number format used for backward quantization.
if is None, the quantization would be a identity mapping.</p></li>
<li><p><strong>forward_rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, &quot;stochastic&quot; or &quot;nearest&quot; (default: &quot;stochastic&quot;)</p></li>
<li><p><strong>backward_rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, &quot;stochastic&quot; or &quot;nearest&quot; (default: &quot;stochastic&quot;)</p></li>
<li><p><strong>clamping_grad_zero</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – zero out the gradient of numbers that are being clamped during forward propagation.
currently requires forward_number to be a fixed point number.</p></li>
<li><p><strong>backward_hooks</strong> – iterable of functions that will be applied to gradients before backward quantization.
For example, this can be used to support custom scaling.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A quantization function as specified</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.qlinear_mp">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.</span></span><span class="sig-name descname"><span class="pre">qlinear_mp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear_mp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.qlinear_mp" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.qlinear_mp.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear_mp.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.qlinear_mp.backward" title="Link to this definition">#</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#mptorch.quant.qlinear_mp.forward" title="mptorch.quant.qlinear_mp.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#mptorch.quant.qlinear_mp.forward" title="mptorch.quant.qlinear_mp.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#mptorch.quant.qlinear_mp.backward" title="mptorch.quant.qlinear_mp.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#mptorch.quant.qlinear_mp.forward" title="mptorch.quant.qlinear_mp.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.qlinear_mp.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mans</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear_mp.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.qlinear_mp.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <span class="xref std std-ref">combining-forward-context</span> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">output</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context()</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <span class="xref std std-ref">extending-autograd</span> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward()</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward()</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-mptorch.quant.quant_format">
<span id="quantization-formats"></span><h2>Quantization Formats<a class="headerlink" href="#module-mptorch.quant.quant_format" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.quant_format.QAffineFormats">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.quant_format.</span></span><span class="sig-name descname"><span class="pre">QAffineFormats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_mac=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_mac=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_rnd=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_rnd=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compensated=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_scaling=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_scaled_format=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaled_format=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_scaled_format=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_margin=0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rbits=0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s=None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_format.html#QAffineFormats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.quant_format.QAffineFormats" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration class for number formats to use during compute (forward
and/or backward pass) of affine layers (e.g. linear and convolutional).
One can optionally specify quantizer objects for the signals in the
layer (I/O activations, weights/bias terms and weight/error gradients)
to facilitate quantization-aware-training (QAT) and post-training
quantization (PTQ) workloads. Format parameters can also be specified
for tensor scaling operations, in a similar way to what is described
in: <a class="reference external" href="https://arxiv.org/pdf/2309.17224">https://arxiv.org/pdf/2309.17224</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_mac</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>] | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>, <a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration (add and multiply) for forward MAC operations</p></li>
<li><p><strong>bwd_mac</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>] | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>, <a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration (add and multiply) for backward MAC operations</p></li>
<li><p><strong>fwd_rnd</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – rounding mode for FWD computations</p></li>
<li><p><strong>bwd_rnd</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – rounding mode for BWD computations</p></li>
<li><p><strong>weight_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]</span>) – quantization function or format and rounding on the weight signal inputs</p></li>
<li><p><strong>bias_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]</span>) – quantization function or format and rounding on the bias signal inputs</p></li>
<li><p><strong>input_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]</span>) – quantization function or format and rounding on the output signal from the layer</p></li>
<li><p><strong>grad_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Union</span></code>[<code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-data docutils literal notranslate"><span class="pre">Tuple</span></code>[<a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>]]</span>) – quantization function or format and rounding on the gradient signals in the BWD pass</p></li>
<li><p><strong>use_scaling</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether to use weight, input and grad scaling during forward/backward pass</p></li>
<li><p><strong>weight_scaled_format</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.FloatType" title="mptorch.number.FloatType"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – number format to be used during weight tensor scaling (optional, matches weight_quant if format specified)</p></li>
<li><p><strong>input_scaled_format</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.FloatType" title="mptorch.number.FloatType"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – number format to be used during input tensor scaling (optional, matches input_quant if format specified)</p></li>
<li><p><strong>grad_scaled_format</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.FloatType" title="mptorch.number.FloatType"><code class="xref py py-class docutils literal notranslate"><span class="pre">FloatType</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – number format to be used during output tensor scaling (optional, matches grad_quant if format specified)</p></li>
<li><p><strong>scale_margin</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – margin to reduce scaling bias when casting down to FP8 formats</p></li>
<li><p><strong>rbits</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – number of bits used for random number generation when rounding is stochastic (for add and multiply)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.quant_format.QSoftmaxFormats">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.quant_format.</span></span><span class="sig-name descname"><span class="pre">QSoftmaxFormats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_off=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_exp=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_acc=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_lse=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_add=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_mul=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_rnd='nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_rnd='nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_format.html#QSoftmaxFormats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.quant_format.QSoftmaxFormats" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration class for number formats to use during compute (forward
and/or backward pass) of softmax layers.
One can optionally specify quantizer objects for the signals in the
layer (I/O activations and weight/error gradients)
to facilitate quantization-aware-training (QAT) and post-training
quantization (PTQ) workloads.</p>
<p>Two implementations of the forward softmax are provided: regular and
LogSumExp-based (LSE).</p>
<p>Regular softmax is used when <cite>fwd_off</cite>, <cite>fwd_exp</cite> and <cite>fwd_acc</cite> are
set, and is implemented as follows:</p>
<div class="math notranslate nohighlight">
\[\textrm{softmax}(x)_i = \frac{\exp(x_i - \max x)}{
    \sum_j \exp(x_j - \max x)}\]</div>
<p>The LogSumExp implementation is used when <cite>fwd_off</cite> and <cite>fwd_exp</cite> are
set, and is implemented as follows:</p>
<div class="math notranslate nohighlight">
\[\textrm{softmax}(x)_i = \ln(\textrm{LSE}(x_1, ..., x_n))\]</div>
<p>where <span class="math notranslate nohighlight">\(\textrm{LSE}(x_1, ..., x_n)\)</span> is computed iteratively
with the relation:</p>
<div class="math notranslate nohighlight">
\[\textrm{LSE}(x_1, ..., x_{j+1})
    = \ln(\exp \textrm{LSE}(x_1, ..., x_{j}) + \exp x_{j+1})\]</div>
<p>with the internal part of the log being computed at full precision.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_off</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward subtraction</p></li>
<li><p><strong>fwd_exp</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward exponential operations</p></li>
<li><p><strong>fwd_acc</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward add operations</p></li>
<li><p><strong>fwd_lse</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward LSE iteration</p></li>
<li><p><strong>bwd_add</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for backward add operations</p></li>
<li><p><strong>bwd_mul</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for backward multiply operations</p></li>
<li><p><strong>fwd_rnd</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – rounding mode for forward computations</p></li>
<li><p><strong>bwd_rnd</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – rounding mode for backward computations</p></li>
<li><p><strong>input_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the input signal</p></li>
<li><p><strong>output_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the output signal</p></li>
<li><p><strong>grad_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the gradients</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.quant_format.QLayerNormFormats">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.quant_format.</span></span><span class="sig-name descname"><span class="pre">QLayerNormFormats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_acc=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_mul=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_div=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_sqrt=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_acc=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_mul=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_div=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_rnd='nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_rnd='nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_format.html#QLayerNormFormats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.quant_format.QLayerNormFormats" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration class for number formats to use during compute (forward
and/or backward pass) of layer normalization.
One can optionally specify quantizer objects for the signals in the
layer (I/O activations, weights/bias terms and weight/error gradients)
to facilitate quantization-aware-training (QAT) and post-training
quantization (PTQ) workloads.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_acc</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward add operations</p></li>
<li><p><strong>fwd_mul</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward multiply operations</p></li>
<li><p><strong>fwd_div</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward divide operations</p></li>
<li><p><strong>fwd_sqrt</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for forward square root operations</p></li>
<li><p><strong>bwd_acc</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for backward add operations</p></li>
<li><p><strong>bwd_mul</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for backward multiply operations</p></li>
<li><p><strong>bwd_div</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – compute configuration for backward divide operations,</p></li>
<li><p><strong>fwd_rnd</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – rounding mode for forward computations</p></li>
<li><p><strong>bwd_rnd</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – rounding mode for backward computations</p></li>
<li><p><strong>input_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the input signal</p></li>
<li><p><strong>output_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the output signal</p></li>
<li><p><strong>grad_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the gradients</p></li>
<li><p><strong>weight_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the weights when applied to an input</p></li>
<li><p><strong>bias_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the bias when applied to an input</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.quant_format.QGELUFormats">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.quant_format.</span></span><span class="sig-name descname"><span class="pre">QGELUFormats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inter_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/quant_format.html#QGELUFormats"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.quant_format.QGELUFormats" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Configuration class for number formats to use during compute (forward
and/or backward pass) of GELU activation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the input signal</p></li>
<li><p><strong>inter_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on intermediate
computation, depends on wether <em>tanh</em> approximation is used</p></li>
<li><p><strong>output_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the output signal</p></li>
<li><p><strong>grad_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function on the gradients</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-mptorch.quant.modules">
<span id="modules"></span><h2>Modules<a class="headerlink" href="#module-mptorch.quant.modules" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.Quantizer">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">Quantizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">forward_number</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_number</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nearest'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_rounding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'nearest'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/quantizer.html#Quantizer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.Quantizer" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>A quantization module that supports quantizing forward and backward process differently.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>forward_number</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the number format used for forward quantization.
if is None, the quantization would be a identity mapping.</p></li>
<li><p><strong>backward_number</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.number.Number" title="mptorch.number.Number"><code class="xref py py-class docutils literal notranslate"><span class="pre">Number</span></code></a> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the number format used for backward quantization.
if is None, the quantization would be a identity mapping.</p></li>
<li><p><strong>forward_rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, &quot;stochastic&quot; or &quot;nearest&quot; (default: &quot;stochastic&quot;)</p></li>
<li><p><strong>backward_rounding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'nearest'</span></code>, <code class="docutils literal notranslate"><span class="pre">'stochastic'</span></code>]</span>) – rounding mode, &quot;stochastic&quot; or &quot;nearest&quot; (default: &quot;stochastic&quot;)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.Quantizer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/quantizer.html#Quantizer.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.Quantizer.forward" title="Link to this definition">#</a></dt>
<dd><p>The forward pass call on the input tensor. Applies the forward quantization on the input and
registers the backward quantization format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The quantized version of the input, as specified by the FWD number format and associated rounding mode</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinear" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></p>
<p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y=xW^T + b\)</span></p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
and is helpful in studying the effect of low precision compute during inference
and training (not just data quantization).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_features</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – size of each input sample</p></li>
<li><p><strong>out_features</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – size of each output sample</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((*, H_\text{in})\)</span> where <span class="math notranslate nohighlight">\(*\)</span> means any number of
dimensions including none and <span class="math notranslate nohighlight">\(H_\text{in} = \text{in_features}\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((*, H_\text{out})\)</span> where all but the last dimension
are the same shape as the input and <span class="math notranslate nohighlight">\(H_\text{out} = \text{out_features}\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QLinear.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out_features}, \text{in_features})\)</span>. The values are
initialized from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math notranslate nohighlight">\(k = \frac{1}{\text{in_features}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QLinear.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape <span class="math notranslate nohighlight">\((\text{out_features})\)</span>.
If <a class="reference internal" href="#mptorch.quant.modules.QLinear.bias" title="mptorch.quant.modules.QLinear.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{1}{\text{in_features}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinear.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor over which to perform the layer operations.
Must adhere to the input shape requirements.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the <span class="math notranslate nohighlight">\(xW^T + b\)</span> operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinear.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinear.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinear.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinear.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QLinear.weight" title="mptorch.quant.modules.QLinear.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QLinear.bias" title="mptorch.quant.modules.QLinear.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinear.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinear.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinear.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLazyLinear">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QLazyLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">out_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLazyLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLazyLinear" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LazyModuleMixin</span></code>, <a class="reference internal" href="#mptorch.quant.modules.QLinear" title="mptorch.quant.modules.linear.QLinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">QLinear</span></code></a></p>
<p>A linear module where <cite>in_features</cite> is inferred.</p>
<p>In this module (an analogue to <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.LazyLinear</span></code>), the
<code class="docutils literal notranslate"><span class="pre">in_features</span></code> parameter of the quantized linear layer is inferred
from the input’s last dimension (i.e., <code class="docutils literal notranslate"><span class="pre">input.shape[-1]</span></code>). The <cite>weight</cite>
and <cite>bias</cite> layer parameters are of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.UninitializedParameter</span></code>
class. They are initialized after the first call to <code class="docutils literal notranslate"><span class="pre">forward</span></code> and the
module becomes a regular <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out_features</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – size of each output sample</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the layer will not learn an additive bias.
Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
</dl>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLazyLinear.cls_to_become">
<span class="sig-name descname"><span class="pre">cls_to_become</span></span><a class="headerlink" href="#mptorch.quant.modules.QLazyLinear.cls_to_become" title="Link to this definition">#</a></dt>
<dd><p>alias of <a class="reference internal" href="#mptorch.quant.modules.QLinear" title="mptorch.quant.modules.linear.QLinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">QLinear</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLazyLinear.initialize_parameters">
<span class="sig-name descname"><span class="pre">initialize_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLazyLinear.initialize_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLazyLinear.initialize_parameters" title="Link to this definition">#</a></dt>
<dd><p>Initialize parameters according to the input
batch properties.</p>
<p>This adds an interface to isolate parameter initialization from the forward
pass when doing parameter shape inference.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations. It’s shape
is used to determine the value of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_features</span></code> member variable.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLazyLinear.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLazyLinear.reset_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLazyLinear.reset_parameters" title="Link to this definition">#</a></dt>
<dd><p>Resets parameter values in case parameters have been initialized</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLazyLinear.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">UninitializedParameter</span></code></em><a class="headerlink" href="#mptorch.quant.modules.QLazyLinear.weight" title="Link to this definition">#</a></dt>
<dd><p>The learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out_features}, \text{in_features})\)</span>. The values are
initialized from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span>, where
<span class="math notranslate nohighlight">\(k = \frac{1}{\text{in_features}}\)</span></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLazyLinear.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><code class="xref py py-class docutils literal notranslate"><span class="pre">UninitializedParameter</span></code></em><a class="headerlink" href="#mptorch.quant.modules.QLazyLinear.bias" title="Link to this definition">#</a></dt>
<dd><p>The learnable bias of the module of shape <span class="math notranslate nohighlight">\((\text{out_features})\)</span>.
If <a class="reference internal" href="#mptorch.quant.modules.QLazyLinear.bias" title="mptorch.quant.modules.QLazyLinear.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the values are initialized from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{1}{\text{in_features}}\)</span></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinearMP">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QLinearMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_activation=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kappa_phi_f=&lt;function</span> <span class="pre">kappa_phi&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kappa_v_f=&lt;function</span> <span class="pre">kappa_v_mp&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol=0.1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinearMP"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinearMP" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Linear</span></code></p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinearMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinearMP.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinearMP.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinearMP.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinearMP.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinearMP.quant_function" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinearMP.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinearMP.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinearMP.quant_parameters" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLinearMP.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/linear.html#QLinearMP.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLinearMP.reset_quant_function" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QConv1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv1d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv1d</span></code></p>
<p>Applies a 1D convolution over an input signal composed of several input planes.</p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv1d</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
(which are performed using the <cite>im2col</cite> and <cite>col2im</cite> algorithms implemented in the
<a class="reference external" href="https://github.com/f-dangel/unfoldNd">unfoldNd</a> library) and is helpful in studying the effect of low precision compute
during inference and training (not just data quantization).</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{\text{in}}, L)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{\text{out}}, L_{\text{out}})\)</span> can be
precisely described as:</p>
<div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_\text{in} - 1} \text{weight}(C_{\text{out}_j}, k)
\star \text{input}(N_i, k)\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(L\)</span> is a length of signal sequence.</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a one-element tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of padding applied to the input. It
can be either a string <cite>valid</cite> or <cite>same</cite> or a tuple of ints giving the
amount of implicit padding applied on both sides.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>.
For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also known as a “depthwise convolution”.</p>
<p>In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_\text{in}, L_\text{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite> can be performed with the arguments
<span class="math notranslate nohighlight">\((C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})\)</span>.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_\text{in}, L_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C_\text{in}, L_\text{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_\text{out}, L_\text{out})\)</span> or <span class="math notranslate nohighlight">\((C_\text{out}, L_\text{out})\)</span>, where</p>
<div class="math notranslate nohighlight">
\[L_\text{out} = \left\lfloor\frac{L_\text{in} + 2 \times \text{padding} - \text{dilation}
          \times (\text{kernel_size} - 1) - 1}{\text{stride}} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QConv1d.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out_channels},
\frac{\text{in_channels}}{\text{groups}}, \text{kernel_size})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{in} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QConv1d.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape
(out_channels). If <a class="reference internal" href="#mptorch.quant.modules.QConv1d.bias" title="mptorch.quant.modules.QConv1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{in} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Size of the convolving kernel</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – Number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Padding added to both sides of the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv1d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv1d.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations.
Must adhere to the input shape requirements.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the 1D cross-correlation operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv1d.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv1d.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv1d.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv1d.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QConv1d.weight" title="mptorch.quant.modules.QConv1d.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QConv1d.bias" title="mptorch.quant.modules.QConv1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv1d.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv1d.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv1d.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QConv2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv2d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv2d</span></code></p>
<p>Applies a 2D convolution over an input signal composed of several input
planes.</p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
(which are performed using the <cite>im2col</cite> and <cite>col2im</cite> algorithms implemented in the
<a class="reference external" href="https://github.com/f-dangel/unfoldNd">unfoldNd</a> library) and is helpful in studying the effect of low precision compute
during inference and training (not just data quantization).</p>
<p>In the simplest case, the output value of the layer with input size
<span class="math notranslate nohighlight">\((N, C_{\text{in}}, H, W)\)</span> and output <span class="math notranslate nohighlight">\((N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
\sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 2D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(H\)</span> is a height of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is
width in pixels.</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of padding applied to the input. It
can be either a string {{‘valid’, ‘same’}} or an int / a tuple of ints giving the
amount of implicit padding applied on both sides.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>.
For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also known as a “depthwise convolution”.</p>
<p>In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_\text{in}, L_\text{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite> can be performed with the arguments
<span class="math notranslate nohighlight">\((C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_\text{in}, H_\text{in}, W_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C_\text{in}, H_\text{in}, W_\text{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_\text{out}, H_\text{out}, W_\text{out})\)</span> or <span class="math notranslate nohighlight">\((C_\text{out}, H_\text{out}, W_\text{out})\)</span>, where</p>
<div class="math notranslate nohighlight">
\[H_\text{out} = \left\lfloor\frac{H_\text{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div>
<div class="math notranslate nohighlight">
\[W_\text{out} = \left\lfloor\frac{W_\text{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QConv2d.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QConv2d.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape
(out_channels). If <a class="reference internal" href="#mptorch.quant.modules.QConv2d.bias" title="mptorch.quant.modules.QConv2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{in} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Size of the convolving kernel</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – Number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Padding added to all four sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv2d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv2d.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations.
Must adhere to the input shape requirements.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the 2D cross-correlation operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv2d.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv2d.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv2d.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv2d.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QConv2d.weight" title="mptorch.quant.modules.QConv2d.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QConv2d.bias" title="mptorch.quant.modules.QConv2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv2d.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv2d.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv2d.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QConv3d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv3d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv3d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Conv3d</span></code></p>
<p>Applies a 3D convolution over an input signal composed of several input
planes.</p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv3d</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
(which are performed using the <cite>im2col</cite> and <cite>col2im</cite> algorithms implemented in the
<a class="reference external" href="https://github.com/f-dangel/unfoldNd">unfoldNd</a> library) and is helpful in studying the effect of low precision compute
during inference and training (not just data quantization).</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C_{\text{in}}, D, H, W)\)</span>
and output <span class="math notranslate nohighlight">\((N, C_{\text{out}}, D_{\text{out}}, H_{\text{out}}, W_{\text{out}})\)</span> can be
precisely described as:</p>
<div class="math notranslate nohighlight">
\[\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
                        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)\]</div>
<p>where <span class="math notranslate nohighlight">\(\star\)</span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator,
<span class="math notranslate nohighlight">\(N\)</span> is a batch size, <span class="math notranslate nohighlight">\(C\)</span> denotes a number of channels,
<span class="math notranslate nohighlight">\(D\)</span> is a depth of input planes in pixels, <span class="math notranslate nohighlight">\(H\)</span> is a height
of input planes in pixels, and <span class="math notranslate nohighlight">\(W\)</span> is width in pixels.</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation, a single
number or a tuple.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of padding applied to the input. It
can be either a string {{‘valid’, ‘same’}} or an int / a tuple of ints giving the
amount of implicit padding applied on both sides.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also
known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a>
has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code> controls the connections between inputs and outputs.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>.
For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When <cite>groups == in_channels</cite> and <cite>out_channels == K * in_channels</cite>,
where <cite>K</cite> is a positive integer, this operation is also known as a “depthwise convolution”.</p>
<p>In other words, for an input of size <span class="math notranslate nohighlight">\((N, C_\text{in}, L_\text{in})\)</span>,
a depthwise convolution with a depthwise multiplier <cite>K</cite> can be performed with the arguments
<span class="math notranslate nohighlight">\((C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">padding='valid'</span></code> is the same as no padding. <code class="docutils literal notranslate"><span class="pre">padding='same'</span></code> pads
the input so the output has the shape as the input. However, this mode
doesn’t support any stride values other than 1.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_\text{in}, D_\text{in}, H_\text{in}, W_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C_\text{in}, D_\text{in}, H_\text{in}, W_\text{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_\text{out}, D_\text{out}, H_\text{out}, W_\text{out})\)</span> or <span class="math notranslate nohighlight">\((C_\text{out}, D_\text{out}, H_\text{out}, W_\text{out})\)</span>,
where</p>
<div class="math notranslate nohighlight">
\[D_\text{out} = \left\lfloor\frac{D_\text{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
      \times (\text{kernel_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor\]</div>
<div class="math notranslate nohighlight">
\[H_\text{out} = \left\lfloor\frac{H_\text{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
      \times (\text{kernel_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor\]</div>
<div class="math notranslate nohighlight">
\[W_\text{out} = \left\lfloor\frac{W_\text{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
      \times (\text{kernel_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QConv3d.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{out_channels}, \frac{\text{in_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{in} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QConv3d.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels). If <a class="reference internal" href="#mptorch.quant.modules.QConv3d.bias" title="mptorch.quant.modules.QConv3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>,
then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{in} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Size of the convolving kernel</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – Number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Padding added to all four sides of
the input. Default: 0</p></li>
<li><p><strong>dilation</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Spacing between kernel elements. Default: 1</p></li>
<li><p><strong>groups</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of blocked connections from input
channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the
output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>padding_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code></span>) – <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code>, <code class="docutils literal notranslate"><span class="pre">'reflect'</span></code>,
<code class="docutils literal notranslate"><span class="pre">'replicate'</span></code> or <code class="docutils literal notranslate"><span class="pre">'circular'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv3d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv3d.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations.
Must adhere to the input shape requirements.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the 3D cross-correlation operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv3d.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv3d.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv3d.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv3d.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QConv3d.weight" title="mptorch.quant.modules.QConv3d.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QConv3d.bias" title="mptorch.quant.modules.QConv3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConv3d.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConv3d.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConv3d.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QConvTranspose1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose1d</span></code></p>
<p>Applies a 1D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv1d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation as it does
not compute a true inverse of convolution). For more information, see the visualizations
<a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> and the <a class="reference external" href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf">Deconvolutional Networks</a> paper.</p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConvTranspose1d</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
(which are performed using the <cite>im2col</cite> and <cite>col2im</cite> algorithms implemented in the
<a class="reference external" href="https://github.com/f-dangel/unfoldNd">unfoldNd</a> library) and is helpful in studying the effect of low precision compute
during inference and training (not just data quantization).</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero padding on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but the link <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>.
For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv1d</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConvTranspose1d</span></code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv1d</span></code> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_\text{in}, L_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C_\text{in}, L_\text{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_\text{out}, L_\text{out})\)</span> or <span class="math notranslate nohighlight">\((C_\text{out}, L_\text{out})\)</span>, where</p>
<div class="math notranslate nohighlight">
\[L_\text{out} = (L_\text{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
          \times (\text{kernel_size} - 1) + \text{output_padding} + 1\]</div>
</li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{out} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape (<cite>out_channels</cite>).
If <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose1d.bias" title="mptorch.quant.modules.QConvTranspose1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{out} * \text{kernel_size}}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Size of the convolving kernel</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – Number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Additional size added to one side
of the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose1d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations.
Must adhere to the input shape requirements.</p></li>
<li><p><strong>output_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – specifies how the output should be padded</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the 1D transposed convolution operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose1d.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose1d.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose1d.weight" title="mptorch.quant.modules.QConvTranspose1d.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose1d.bias" title="mptorch.quant.modules.QConvTranspose1d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose1d.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose1d.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose1d.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QConvTranspose2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose2d</span></code></p>
<p>Applies a 2D transposed convolution operator over an input image
composed of several input planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation as it does
not compute a true inverse of convolution). For more information, see the visualizations
<a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> and the <a class="reference external" href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf">Deconvolutional Networks</a> paper.</p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConvTranspose2d</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
(which are performed using the <cite>im2col</cite> and <cite>col2im</cite> algorithms implemented in the
<a class="reference external" href="https://github.com/f-dangel/unfoldNd">unfoldNd</a> library) and is helpful in studying the effect of low precision compute
during inference and training (not just data quantization).</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero padding on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but the link <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>.
For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimensions</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConvTranspose2d</span></code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv2d</span></code> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_\text{in}, H_\text{in}, W_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C_\text{in}, H_\text{in}, W_\text{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_\text{out}, H_\text{out}, W_\text{out})\)</span> or <span class="math notranslate nohighlight">\((C_\text{out}, H_\text{out}, W_\text{out})\)</span>, where</p></li>
</ul>
<div class="math notranslate nohighlight">
\[H_\text{out} = (H_\text{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\]</div>
<div class="math notranslate nohighlight">
\[W_\text{out} = (W_\text{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\]</div>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{out} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels)
If <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose2d.bias" title="mptorch.quant.modules.QConvTranspose2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{out} * \prod_{i=0}^{1}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Size of the convolving kernel</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – Number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Additional size added to one side
of the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose2d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations.
Must adhere to the input shape requirements.</p></li>
<li><p><strong>output_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – specifies how the output should be padded</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the 2D transposed convolution operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose2d.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose2d.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose2d.weight" title="mptorch.quant.modules.QConvTranspose2d.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose2d.bias" title="mptorch.quant.modules.QConvTranspose2d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose2d.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose2d.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose2d.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QConvTranspose3d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'zeros'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose3d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">ConvTranspose3d</span></code></p>
<p>Applies a 3D transposed convolution operator over an input image
composed of several input planes. The transposed convolution operator multiplies
each input value element-wise by a learnable kernel, and sums over the outputs
from all input feature planes.</p>
<p>This module can be seen as the gradient of Conv2d with respect to its input.
It is also known as a fractionally-strided convolution or
a deconvolution (although it is not an actual deconvolution operation as it does
not compute a true inverse of convolution). For more information, see the visualizations
<a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> and the <a class="reference external" href="https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf">Deconvolutional Networks</a> paper.</p>
<p>It is a subclass of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConvTranspose3d</span></code> and allows one to specify if I/O
signals should be quantized during inference &amp; training (needed for instance
in QAT and PTQ methods), but also the precision(s) to be used in internal GEMM
computations (addition and multiplication, fused or not). This allows simulating
the effect of custom precision during GEMM calls in the forward and backward pass
(which are performed using the <cite>im2col</cite> and <cite>col2im</cite> algorithms implemented in the
<a class="reference external" href="https://github.com/f-dangel/unfoldNd">unfoldNd</a> library) and is helpful in studying the effect of low precision compute
during inference and training (not just data quantization).</p>
<ul>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code> controls the stride for the cross-correlation.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> controls the amount of implicit zero padding on both
sides for <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> number of points. See note
below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> controls the additional size added to one side
of the output shape. See note below for details.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> controls the spacing between the kernel points; also known as the à trous algorithm.
It is harder to describe, but the link <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">here</a> has a nice visualization of what <code class="xref py py-attr docutils literal notranslate"><span class="pre">dilation</span></code> does.</p></li>
<li><p><code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out_channels</span></code> must both be divisible by <code class="xref py py-attr docutils literal notranslate"><span class="pre">groups</span></code>.
For example,</p>
<blockquote>
<div><ul class="simple">
<li><p>At groups=1, all inputs are convolved to all outputs.</p></li>
<li><p>At groups=2, the operation becomes equivalent to having two conv
layers side by side, each seeing half the input channels
and producing half the output channels, and both subsequently
concatenated.</p></li>
<li><p>At groups= <code class="xref py py-attr docutils literal notranslate"><span class="pre">in_channels</span></code>, each input channel is convolved with
its own set of filters (of size
<span class="math notranslate nohighlight">\(\frac{\text{out_channels}}{\text{in_channels}}\)</span>).</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code>
can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the depth, height and width dimensions</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of three ints – in which case, the first <cite>int</cite> is used for the depth dimension,
the second <cite>int</cite> for the height dimension and the third <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> argument effectively adds <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code>
amount of zero padding to both sizes of the input. This is set so that
when a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv3d</span></code> and a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ConvTranspose3d</span></code>
are initialized with same parameters, they are inverses of each other in
regard to the input and output shapes. However, when <code class="docutils literal notranslate"><span class="pre">stride</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>,
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Conv3d</span></code> maps multiple input shapes to the same output
shape. <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is provided to resolve this ambiguity by
effectively increasing the calculated output shape on one side. Note
that <code class="xref py py-attr docutils literal notranslate"><span class="pre">output_padding</span></code> is only used to find output shape, but does
not actually add zero-padding to output.</p>
</div>
<dl>
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C_\text{in}, D_\text{in}, H_\text{in}, W_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C_\text{in}, D_\text{in}, H_\text{in}, W_\text{in})\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C_\text{out}, D_\text{out}, H_\text{out}, W_\text{out})\)</span> or
<span class="math notranslate nohighlight">\((C_\text{out}, D_\text{out}, H_\text{out}, W_\text{out})\)</span>, where</p></li>
</ul>
<div class="math notranslate nohighlight">
\[D_\text{out} = (D_\text{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
          \times (\text{kernel_size}[0] - 1) + \text{output_padding}[0] + 1\]</div>
<div class="math notranslate nohighlight">
\[H_\text{out} = (H_\text{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
          \times (\text{kernel_size}[1] - 1) + \text{output_padding}[1] + 1\]</div>
<div class="math notranslate nohighlight">
\[W_\text{out} = (W_\text{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
          \times (\text{kernel_size}[2] - 1) + \text{output_padding}[2] + 1\]</div>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\((\text{in_channels}, \frac{\text{out_channels}}{\text{groups}},\)</span>
<span class="math notranslate nohighlight">\(\text{kernel_size[0]}, \text{kernel_size[1]}, \text{kernel_size[2]})\)</span>.
The values of these weights are sampled from
<span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{out} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape (out_channels)
If <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose3d.bias" title="mptorch.quant.modules.QConvTranspose3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the values of these weights are
sampled from <span class="math notranslate nohighlight">\(\mathcal{U}(-\sqrt{k}, \sqrt{k})\)</span> where
<span class="math notranslate nohighlight">\(k = \frac{\text{groups}}{C_\text{out} * \prod_{i=0}^{2}\text{kernel_size}[i]}\)</span></p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels in the input image</p></li>
<li><p><strong>out_channels</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of channels produced by the convolution</p></li>
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Size of the convolving kernel</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – Number formats used during compute (addition and multiplication) and
quantization functions for signals during forward and back propagation (I/O
activations, weights, biases, and neural gradients)</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Stride of the convolution. Default: 1</p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – <code class="docutils literal notranslate"><span class="pre">dilation</span> <span class="pre">*</span> <span class="pre">(kernel_size</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">-</span> <span class="pre">padding</span></code> zero-padding
will be added to both sides of the input. Default: 0</p></li>
<li><p><strong>output_padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Additional size added to one side
of the output shape. Default: 0</p></li>
<li><p><strong>groups</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – Number of blocked connections from input channels to output channels. Default: 1</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, adds a learnable bias to the output. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>dilation</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – Spacing between kernel elements. Default: 1</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose3d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d.forward" title="Link to this definition">#</a></dt>
<dd><p>Describes the computations that get performed at every call of the module. The use
of quantized elementary operations (i.e., additions and multiplications) in the FWD
and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor on which to perform the layer operations.
Must adhere to the input shape requirements.</p></li>
<li><p><strong>output_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – specifies how the output should be padded</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the 3D transposed convolution operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose3d.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose3d.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose3d.weight" title="mptorch.quant.modules.QConvTranspose3d.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QConvTranspose3d.bias" title="mptorch.quant.modules.QConvTranspose3d.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QConvTranspose3d.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/conv.html#QConvTranspose3d.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QConvTranspose3d.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QAvgPool2d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QAvgPool2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ceil_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">count_include_pad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">divisor_override</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/pooling.html#QAvgPool2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QAvgPool2d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AvgPool2d</span></code></p>
<p>Applies a 2D average pooling over an input signal composed of several input planes. Performs
the addition operations in the FWD and BWD passes using quantized operators given as parameters.</p>
<p>In the simplest case, the output value of the layer with input size <span class="math notranslate nohighlight">\((N, C, H, W)\)</span>,
output <span class="math notranslate nohighlight">\((N, C, H_\text{out}, W_\text{out})\)</span> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code> <span class="math notranslate nohighlight">\((kH, kW)\)</span>
can be precisely described as:</p>
<div class="math notranslate nohighlight">
\[\text{out}(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                       \text{input}(N_i, C_j, \text{stride}[0] \times h + m, \text{stride}[1] \times w + n)\]</div>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> is non-zero, then the input is implicitly zero-padded on both sides
for <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> number of points.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When ceil_mode=True, sliding windows are allowed to go off-bounds if they start within the left padding
or the input. Sliding windows that would start in the right padded region are ignored.</p>
</div>
<p>The parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">stride</span></code>, <code class="xref py py-attr docutils literal notranslate"><span class="pre">padding</span></code> can either be:</p>
<blockquote>
<div><ul class="simple">
<li><p>a single <code class="docutils literal notranslate"><span class="pre">int</span></code> – in which case the same value is used for the height and width dimension</p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">tuple</span></code> of two ints – in which case, the first <cite>int</cite> is used for the height dimension,
and the second <cite>int</cite> for the width dimension</p></li>
</ul>
</div></blockquote>
<dl>
<dt>Shape:</dt><dd><ul>
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H_\text{in}, W_\text{in})\)</span> or <span class="math notranslate nohighlight">\((C, H_\text{in}, W_\text{in})\)</span>.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H_\text{out}, W_\text{out})\)</span> or <span class="math notranslate nohighlight">\((C, H_\text{out}, W_\text{out})\)</span>, where</p>
<div class="math notranslate nohighlight">
\[H_\text{out} = \left\lfloor\frac{H_\text{in}  + 2 \times \text{padding}[0] -
  \text{kernel_size}[0]}{\text{stride}[0]} + 1\right\rfloor\]</div>
<div class="math notranslate nohighlight">
\[W_\text{out} = \left\lfloor\frac{W_\text{in}  + 2 \times \text{padding}[1] -
  \text{kernel_size}[1]}{\text{stride}[1]} + 1\right\rfloor\]</div>
<p>Per the note above, if <code class="docutils literal notranslate"><span class="pre">ceil_mode</span></code> is True and <span class="math notranslate nohighlight">\((H_\text{out} - 1)\times \text{stride}[0]\geq H_\text{in}
+ \text{padding}[0]\)</span>, we skip the last window as it would start in the bottom padded region,
resulting in <span class="math notranslate nohighlight">\(H_\text{out}\)</span> being reduced by one.</p>
<p>The same applies for <span class="math notranslate nohighlight">\(W_\text{out}\)</span>.</p>
</li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel_size</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – the size of the window</p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function to use during FWD addition operations</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function to use during BWD addition operations</p></li>
<li><p><strong>stride</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the stride of the window. Default value is <code class="xref py py-attr docutils literal notranslate"><span class="pre">kernel_size</span></code></p></li>
<li><p><strong>padding</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]</span>) – implicit zero padding to be added on both sides</p></li>
<li><p><strong>ceil_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – when True, will use <cite>ceil</cite> instead of <cite>floor</cite> to compute the output shape</p></li>
<li><p><strong>count_include_pad</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – when True, will include the zero-padding in the averaging calculation</p></li>
<li><p><strong>divisor_override</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – if specified, it will be used as divisor, otherwise size of the pooling region will be used.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QAvgPool2d.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/pooling.html#QAvgPool2d.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QAvgPool2d.forward" title="Link to this definition">#</a></dt>
<dd><p>Performs the pooling operation over the <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor over which to perform the pooling operation.
Must adhere to the input shape requirements.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the pooling operation.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QBatchNorm1d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QBatchNorm1d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/batchnorm.html#QBatchNorm1d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QBatchNorm1d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">QBatchNorm</span></code></p>
<p>Applies Batch Normalization over a 2D input.</p>
<p>Method described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the number of features or channels of the input). By default, the
elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0.
At train time in the forward pass, the variance is calculated via the biased estimator,
equivalent to <code class="docutils literal notranslate"><span class="pre">torch.var(input,</span> <span class="pre">unbiased=False)</span></code>. However, the value stored in the
moving average of the variance is calculated via the unbiased  estimator, equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.var(input,</span> <span class="pre">unbiased=True)</span></code>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default momentum of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This momentum is different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the batch size,
<span class="math notranslate nohighlight">\(C\)</span> is the number of features or channels, and <span class="math notranslate nohighlight">\(L\)</span> is the sequence length</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C)\)</span> or <span class="math notranslate nohighlight">\((N, C, L)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, L)</cite> slices, it’s common terminology to call this Temporal Batch Normalization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – number of features or channels <span class="math notranslate nohighlight">\(C\)</span> of the input</p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function to use during FWD operations</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function to use during BWD operations</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QBatchNorm2d">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QBatchNorm2d</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/batchnorm.html#QBatchNorm2d"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QBatchNorm2d" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">QBatchNorm</span></code></p>
<p>Applies Batch Normalization over a 4D input.</p>
<p>4D is a mini-batch of 2D inputs
with additional channel dimension. Method described in the paper
<a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing
Internal Covariate Shift</a> .</p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated per-dimension over
the mini-batches and <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable parameter vectors
of size <cite>C</cite> (where <cite>C</cite> is the input size). By default, the elements of <span class="math notranslate nohighlight">\(\gamma\)</span> are set
to 1 and the elements of <span class="math notranslate nohighlight">\(\beta\)</span> are set to 0. At train time in the forward pass, the
standard-deviation is calculated via the biased estimator, equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.var(input,</span> <span class="pre">unbiased=False)</span></code>. However, the value stored in the moving average of the
standard-deviation is calculated via the unbiased  estimator, equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.var(input,</span> <span class="pre">unbiased=True)</span></code>.</p>
<p>Also by default, during training this layer keeps running estimates of its
computed mean and variance, which are then used for normalization during
evaluation. The running estimates are kept with a default momentum
of 0.1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This momentum different from one used in optimizer
classes and the conventional notion of momentum. Mathematically, the
update rule for running statistics here is
<span class="math notranslate nohighlight">\(\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t\)</span>,
where <span class="math notranslate nohighlight">\(\hat{x}\)</span> is the estimated statistic and <span class="math notranslate nohighlight">\(x_t\)</span> is the
new observed value.</p>
</div>
<p>Because the Batch Normalization is done over the <cite>C</cite> dimension, computing statistics
on <cite>(N, H, W)</cite> slices, it’s common terminology to call this Spatial Batch Normalization.</p>
<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, C, H, W)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_features</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – <span class="math notranslate nohighlight">\(C\)</span> from an expected input of size
<span class="math notranslate nohighlight">\((N, C, H, W)\)</span></p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function to use during FWD operations</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – quantization function to use during BWD operations</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QLayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">elementwise_affine</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/layernorm.html#QLayerNorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LayerNorm</span></code></p>
<p>Applies Layer Normalization over a mini-batch of inputs.</p>
<p>This layer implements the operation as described in
the paper <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p>
<div class="math notranslate nohighlight">
\[y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta\]</div>
<p>The mean and standard-deviation are calculated over the last <cite>D</cite> dimensions, where <cite>D</cite>
is the dimension of <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>. For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>
is <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">5)</span></code> (a 2-dimensional shape), the mean and standard-deviation are computed over
the last 2 dimensions of the input (i.e. <code class="docutils literal notranslate"><span class="pre">input.mean((-2,</span> <span class="pre">-1))</span></code>).
<span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable affine transform parameters of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code> if <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The variance is calculated via the biased estimator, equivalent to
<code class="docutils literal notranslate"><span class="pre">torch.var(input,</span> <span class="pre">unbiased=False)</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike Batch Normalization and Instance Normalization, which applies
scalar scale and bias for each entire channel/plane with the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> option, Layer Normalization applies per-element scale and
bias with <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code>.</p>
</div>
<p>This layer uses statistics computed from input data in both training and
evaluation modes.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm.weight">
<span class="sig-name descname"><span class="pre">weight</span></span><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm.weight" title="Link to this definition">#</a></dt>
<dd><p>the learnable weights of the module of shape
<span class="math notranslate nohighlight">\(\text{normalized_shape}\)</span> when <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The values are initialized to 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm.bias">
<span class="sig-name descname"><span class="pre">bias</span></span><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm.bias" title="Link to this definition">#</a></dt>
<dd><p>the learnable bias of the module of shape
<span class="math notranslate nohighlight">\(\text{normalized_shape}\)</span> when <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
The values are initialized to 0.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="simple">
<dt>Shape:</dt><dd><ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\((N, *)\)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\((N, *)\)</span> (same shape as input)</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>normalized_shape</strong> (<em>int</em><em> or </em><em>list</em><em> or </em><em>torch.Size</em>) – <p>input shape from an expected input
of size</p>
<div class="math notranslate nohighlight">
\[[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1]
    \times \ldots \times \text{normalized_shape}[-1]]\]</div>
<p>If a single integer is used, it is treated as a singleton list, and this module will
normalize over the last dimension which is expected to be of that specific size.</p>
</p></li>
<li><p><strong>eps</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – a value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>elementwise_affine</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – a boolean value that when set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, this module
has learnable per-element affine parameters initialized to ones (for weights)
and zeros (for biases). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – If set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the layer will not learn an additive bias (only relevant if
<code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/layernorm.html#QLayerNorm.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm.forward" title="Link to this definition">#</a></dt>
<dd><p>Performs the layernorm operation on the input tensor, using quantized elementary
operations (e.g. additions and multiplications) in the FWD and BWD passes, as specified
through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor over which to perform the layernorm operations.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the output of the layernorm operation</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/layernorm.html#QLayerNorm.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm.quant_parameters">
<span class="sig-name descname"><span class="pre">quant_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/layernorm.html#QLayerNorm.quant_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm.quant_parameters" title="Link to this definition">#</a></dt>
<dd><p>Quantizes the module parameters <a class="reference internal" href="#mptorch.quant.modules.QLayerNorm.weight" title="mptorch.quant.modules.QLayerNorm.weight"><code class="xref py py-attr docutils literal notranslate"><span class="pre">weight</span></code></a> and <a class="reference internal" href="#mptorch.quant.modules.QLayerNorm.bias" title="mptorch.quant.modules.QLayerNorm.bias"><code class="xref py py-attr docutils literal notranslate"><span class="pre">bias</span></code></a> using
the quantization functions specified in <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.weight_quant</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats.bias_quant</span></code>, respectively.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QLayerNorm.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/layernorm.html#QLayerNorm.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QLayerNorm.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized signals in the module (these can be the weight, bias, input,
and/or output signals), depending if quantizers are specified in the
associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QSoftmax">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QSoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/softmax.html#QSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QSoftmax" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Softmax</span></code></p>
<p>A quantized implementation of the Softmax activation function.</p>
<p>This class extends PyTorch’s <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Softmax</span></code> and allows one to specify if I/O
signals and internal computations should be quantized during inference &amp; training.
This allows simulating the effect of custom precision in the internal forward and
backward pass computations and is helpful in studying the effect of low precision
compute during inference and training (not just data quantization).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code></span>) – The dimension along which Softmax will be applied.</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QSoftmaxFormats" title="mptorch.quant.quant_format.QSoftmaxFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QSoftmaxFormats</span></code></a></span>) – Number formats specification during compute
and quantization functions for signals during forward and back propagation
(I/O activations and gradients).</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/softmax.html#QSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QSoftmax.forward" title="Link to this definition">#</a></dt>
<dd><p>Performs the softmax operation on the input tensor. The use of
quantized elementary operations (i.e., additions and multiplications) in
the FWD and BWD passes are controlled through the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> argument
to the module constructor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> – the input tensor over which to perform the softmax operations.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the softmax operation.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QSoftmax.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/softmax.html#QSoftmax.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QSoftmax.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QSoftmax.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/softmax.html#QSoftmax.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QSoftmax.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized I/O signals in the module, depending if quantizers are
specified in the associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QSoftmaxFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.modules.QGELU">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.modules.</span></span><span class="sig-name descname"><span class="pre">QGELU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approximate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'none'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/gelu.html#QGELU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QGELU" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">GELU</span></code></p>
<p>Applies the Gaussian Error Linear Units function to the input <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{GELU}(x) = x * \Phi(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> is the Cumulative Distribution Function for Gaussian Distribution.</p>
<p>When the approximate argument is <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code>, GELU is estimated with:</p>
<div class="math notranslate nohighlight">
\[\text{GELU}(x) = 0.5 * x * (1 + \tanh(\sqrt{2 / \pi} * (x + 0.044715 * x^3)))\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – the input tensor</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QGELUFormats" title="mptorch.quant.quant_format.QGELUFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QGELUFormats</span></code></a></span>) – configuration class for number formats and quantizers to use during
forward and backward computations in GELU</p></li>
<li><p><strong>approximate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'none'</span></code>, <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code>]</span>) – the GELU approximation algorithm to use:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'none'</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QGELU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/gelu.html#QGELU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QGELU.forward" title="Link to this definition">#</a></dt>
<dd><p>Applies the GELU function on the input tensor element-wise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor over which the GELU function is applied</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of applying the GELU function</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QGELU.quant_function">
<span class="sig-name descname"><span class="pre">quant_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/gelu.html#QGELU.quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QGELU.quant_function" title="Link to this definition">#</a></dt>
<dd><p>Defines a straight-through estimator-like function (see <em>Estimating or
Propagating Gradients Through Stochastic Neurons for Conditional Computation</em>
(<a class="reference external" href="https://arxiv.org/abs/1308.3432">https://arxiv.org/abs/1308.3432</a>)) that applies potentially different
quantization functions in the forward and backward passes through the
input and output gradient signals, respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the forward pass
through the input signal</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply during the backward pass
through the output gradient signal</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.modules.QGELU.reset_quant_function">
<span class="sig-name descname"><span class="pre">reset_quant_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/modules/gelu.html#QGELU.reset_quant_function"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.modules.QGELU.reset_quant_function" title="Link to this definition">#</a></dt>
<dd><p>Sets a straight-through estimator-like function to all the
quantized I/O signals in the module, depending if quantizers are
specified in the associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QGELUFormats</span></code> <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-mptorch.quant.functional">
<span id="functional"></span><h2>Functional<a class="headerlink" href="#module-mptorch.quant.functional" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qlinear">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qlinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QAffineFormats(default_fwd,</span> <span class="pre">default_bwd,</span> <span class="pre">rbits_add=0,</span> <span class="pre">rbits_mul=0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qlinear" title="Link to this definition">#</a></dt>
<dd><p>Applies a linear transformation to the incoming data: <span class="math notranslate nohighlight">\(y=xW^T + b\)</span></p>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter allows one to specify if I/O signals should be
quantized during inference &amp; training (needed for instance in QAT and PTQ methods),
but also the precision(s) to be used in internal GEMM computations (addition and
multiplication, fused or not). This allows simulating the effect of custom precision
during GEMM calls in the forward and backward pass and is helpful in studying the
effect of low precision compute during inference and training (not just data
quantization).</p>
<p>This is the functional version of <a class="reference internal" href="#mptorch.quant.modules.QLinear" title="mptorch.quant.modules.QLinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">QLinear</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input <span class="math notranslate nohighlight">\(x\)</span> to the linear layer of the form <span class="math notranslate nohighlight">\((*, H_\text{in})\)</span>,
where <span class="math notranslate nohighlight">\(*\)</span> means any number of dimensions including none and
<span class="math notranslate nohighlight">\(H_{in} = \text{in_features}\)</span></p></li>
<li><p><strong>weight</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the weight tensor <span class="math notranslate nohighlight">\(W\)</span> of shape <span class="math notranslate nohighlight">\((\text{out_features}, \text{in_features})\)</span></p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – optional bias term of shape <span class="math notranslate nohighlight">\((\text{out_features})\)</span></p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – the configuration object for how quantization (if any!) should be handled
on the matrix inputs and how the MAC and summation operations should be performed
(e.g. using compensated algorithms or not)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the affine operation <span class="math notranslate nohighlight">\(xW^T + b\)</span></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.functional.qlinear_mp">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qlinear_mp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear_mp"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qlinear_mp" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.functional.qlinear_mp.forward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mans</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exps</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear_mp.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qlinear_mp.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <span class="xref std std-ref">combining-forward-context</span> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span><span class="w"> </span><span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">output</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context()</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <span class="xref std std-ref">extending-autograd</span> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward()</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward()</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.functional.qlinear_mp.backward">
<em class="property"><span class="k"><span class="pre">static</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlinear_mp.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qlinear_mp.backward" title="Link to this definition">#</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#mptorch.quant.functional.qlinear_mp.forward" title="mptorch.quant.functional.qlinear_mp.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#mptorch.quant.functional.qlinear_mp.forward" title="mptorch.quant.functional.qlinear_mp.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#mptorch.quant.functional.qlinear_mp.backward" title="mptorch.quant.functional.qlinear_mp.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#mptorch.quant.functional.qlinear_mp.forward" title="mptorch.quant.functional.qlinear_mp.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qmatmul">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qmatmul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QAffineFormats(default_fwd,</span> <span class="pre">default_bwd,</span> <span class="pre">rbits_add=0,</span> <span class="pre">rbits_mul=0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qmatmul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qmatmul" title="Link to this definition">#</a></dt>
<dd><p>Simulates a mixed-precision computation pipeline for (batched) matrix multiplication of the
tensors <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code>.</p>
<p>The behavior depends on the dimensionality of the tensors as follows:</p>
<ul>
<li><p>If both tensors are 1-dimensional, the dot product (scalar) is returned.</p></li>
<li><p>If both arguments are 2-dimensional, the matrix-matrix product is returned.</p></li>
<li><p>If the first argument is 1-dimensional and the second argument is 2-dimensional,
a 1 is prepended to its dimension for the purpose of the matrix multiply.
After the matrix multiply, the prepended dimension is removed.</p></li>
<li><p>If the first argument is 2-dimensional and the second argument is 1-dimensional,
the matrix-vector product is returned.</p></li>
<li><p>If both arguments are at least 1-dimensional and at least one argument is
N-dimensional (where 2 &lt; N &lt; 5), then a batched matrix multiply is returned.  If the first
argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
The non-matrix (i.e. batch) dimensions are broadcasted (and thus
must be broadcastable in the PyTorch sense).  For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a
<span class="math notranslate nohighlight">\((j \times 1 \times n \times n)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a <span class="math notranslate nohighlight">\((k \times n \times n)\)</span>
tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a <span class="math notranslate nohighlight">\((j \times k \times n \times n)\)</span> tensor.</p>
<p>Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs
are broadcastable, and not the matrix dimensions. For example, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a
<span class="math notranslate nohighlight">\((j \times 1 \times n \times m)\)</span> tensor and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code> is a <span class="math notranslate nohighlight">\((k \times m \times p)\)</span>
tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the
matrix dimensions) are different. <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> will be a <span class="math notranslate nohighlight">\((j \times k \times n \times p)\)</span> tensor.</p>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the first tensor to be multiplied</p></li>
<li><p><strong>other</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the second tensor to be multiplied</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – the configuration object for how quantization (if any!) should be handled on the tensor inputs
and how the MAC and summation operations should be performed (e.g. using compensated algorithms or not)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the (batched) matrix multiplication between <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">other</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qmm">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qmm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mat2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QAffineFormats(default_fwd,</span> <span class="pre">default_bwd,</span> <span class="pre">rbits_add=0,</span> <span class="pre">rbits_mul=0)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qmm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qmm" title="Link to this definition">#</a></dt>
<dd><p>Simulates a mixed-precision computation pipeline for matrix multiplication of the
matrices <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code>.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> is a <span class="math notranslate nohighlight">\((n \times m)\)</span> tensor, <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code> is a
<span class="math notranslate nohighlight">\((m \times p)\)</span> tensor, the output tensor will be a <span class="math notranslate nohighlight">\((n \times p)\)</span>
tensor.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function does not broadcast. For broadcasting quantized matrix
products, see <a class="reference internal" href="#mptorch.quant.functional.qmatmul" title="mptorch.quant.functional.qmatmul"><code class="xref py py-func docutils literal notranslate"><span class="pre">mptorch.quant.functional.qmatmul()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the first matrix to be multiplied</p></li>
<li><p><strong>mat2</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the second matrix to be multiplied</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QAffineFormats" title="mptorch.quant.quant_format.QAffineFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></span>) – the configuration object for how quantization (if any!) should be handled on the matrix inputs
and how the MAC and summation operations should be performed (e.g. using compensated algorithms or not)</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the result of the matrix multiplication between <code class="xref py py-attr docutils literal notranslate"><span class="pre">input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">mat2</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qadd">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qadd</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qadd"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qadd" title="Link to this definition">#</a></dt>
<dd><p>Adds <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> to <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code>. Uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">fwd_quant</span></code> to quantize the result of the addition
(e.g. can simulate the execution of the addition in low-precision, assuming the inputs are
already in low precision). The <code class="xref py py-attr docutils literal notranslate"><span class="pre">bwd_quant</span></code> function is used to quantize the gradients
from the operator during the backward pass.</p>
<p>For the forward computation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{out} = \mathcal{Q}_\text{fwd}(\text{x} + \text{y})\]</div>
</div></blockquote>
<p>For the backward computation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{grad_x} = \mathcal{Q}_\text{bwd}(\text{grad_z} * \text{ones_like}(\text{x}))\\\text{grad_y} = \mathcal{Q}_\text{bwd}(\text{grad_z} * \text{ones_like}(\text{y}))\end{aligned}\end{align} \]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>y</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the other tensor to add to <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code></p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the forward addition</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the gradient computations in the backward pass</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the addition operation between <cite>x</cite> and <cite>y</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qmul">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qmul</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qmul"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qmul" title="Link to this definition">#</a></dt>
<dd><p>Multiplies <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code>. Uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">fwd_quant</span></code> to quantize the result of the multiplication
(e.g. can simulate the execution of the multiplication in low-precision, assuming the inputs are
already in low precision). The <code class="xref py py-attr docutils literal notranslate"><span class="pre">bwd_quant</span></code> function is used to quantize the gradients
from the operator during the backward pass.</p>
<p>For the forward computation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{out} = \mathcal{Q}_\text{fwd}(\text{x} * \text{y})\]</div>
</div></blockquote>
<p>For the backward computation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{grad_x} = \mathcal{Q}_\text{bwd}(\text{grad_z} * \text{y})\\\text{grad_y} = \mathcal{Q}_\text{bwd}(\text{grad_z} * \text{x})\end{aligned}\end{align} \]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>y</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the other tensor to add to <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code></p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the forward multiplication</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the gradient computations in the backward pass</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the multiplication operation between <cite>x</cite> and <cite>y</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qsqrt">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qsqrt</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qsqrt"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qsqrt" title="Link to this definition">#</a></dt>
<dd><p>Returns a new tensor with the square-root of the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>.</p>
<div class="math notranslate nohighlight">
\[\text{out}_{i} = \sqrt{\text{x}_{i}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the forward square root operation</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the gradient computations in the backward pass</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the square root operation on <cite>x</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qdiv">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qdiv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qdiv"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qdiv" title="Link to this definition">#</a></dt>
<dd><p>Divides <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> by <code class="xref py py-attr docutils literal notranslate"><span class="pre">y</span></code>. Uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">fwd_quant</span></code> to quantize the result of the division
(e.g. can simulate the execution of the division in low-precision, assuming the inputs are
already in low precision). The <code class="xref py py-attr docutils literal notranslate"><span class="pre">bwd_quant</span></code> function is used to quantize the gradients
from the operator during the backward pass.</p>
<p>For the forward computation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{out}_i = \mathcal{Q}_\text{fwd}\left(\frac{\text{x}_i}{\text{y}_i}\right)\]</div>
</div></blockquote>
<p>For the backward computation:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\text{grad_x} = \mathcal{Q}_\text{bwd}\left(\frac{\text{grad_z}}{\text{y}}\right)\\\text{grad_y} = \mathcal{Q}_\text{bwd}\left(\frac{\mathcal{Q}_\text{bwd}\left(-\text{grad_z} * \text{x}\right)}{\text{y}}\right)\end{aligned}\end{align} \]</div>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>y</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the other tensor to add to <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code></p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the forward division</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the gradient computations in the backward pass</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the division operation between <cite>x</cite> and <cite>y</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qpow">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qpow</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qpow"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qpow" title="Link to this definition">#</a></dt>
<dd><p>Takes the power of each element in <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> with <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> and
returns a tensor with the result. <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> can be either a single
<code class="docutils literal notranslate"><span class="pre">float</span></code> number or a <cite>torch.Tensor</cite> with the same number of
elements as <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>.</p>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> is a scalar value, the forward operation applied is:</p>
<div class="math notranslate nohighlight">
\[\text{out}_i = \mathcal{Q}_\text{fwd}\left(\text{x}_i^\text{n}\right)\]</div>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> is a tensor, the operation applied is:</p>
<div class="math notranslate nohighlight">
\[\text{out}_i = \mathcal{Q}_\text{fwd}\left(\text{x}_i^{\text{n}_i}\right)\]</div>
<p>When <code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> is a tensor, the shapes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> and
<code class="xref py py-attr docutils literal notranslate"><span class="pre">n</span></code> must be broadcastable.</p>
<p>The backward operation is (applied element-wise):</p>
<div class="math notranslate nohighlight">
\[\text{out} = \mathcal{Q}_\text{bwd}\left(\text{grad_out} *\mathcal{Q}_\text{bwd}\left(\mathcal{Q}_\text{bwd}\left(\text{x}^{\text{n}-1}\right) * \text{n}\right)\right)\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the forward division</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function to apply on the gradient computations in the backward pass</p></li>
<li><p><strong>n</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – the exponent value</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the power operation between <cite>x</cite> and <cite>n</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qsum">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qsum</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant=&lt;function</span> <span class="pre">&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim=False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qsum"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qsum" title="Link to this definition">#</a></dt>
<dd><p>Returns the quantized sum of all elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> tensor. It can simulate low
precision summation if the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> are low precision values. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">quant</span></code>
function specifies the accumulator output format and precision as a quantization function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the dimension or dimensions to reduce. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all dimensions are reduced</p></li>
<li><p><strong>quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function specifying how accumulation results should be stored</p></li>
<li><p><strong>keepdim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the sum operation operation on <cite>x</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qmean">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qmean</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bwd_quant</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qmean"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qmean" title="Link to this definition">#</a></dt>
<dd><p>Returns the mean value of all the elements in the <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> tensor. Input must be a floating point tensor.
It can simulate low precision summation if the elements of <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code> are low precision values.
The <code class="xref py py-attr docutils literal notranslate"><span class="pre">fwd_quant</span></code> function specifies the accumulator (and final division) output format and
precision as a quantization function in the forward pass. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">bwd_quant</span></code> function specifies how
the arithmetic should be performed in the backward pass through this operator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>, <code class="xref py py-data docutils literal notranslate"><span class="pre">...</span></code>] | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – the dimension or dimensions to reduce. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, all dimensions are reduced</p></li>
<li><p><strong>fwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function specifying how accumulation and division results should be stored in the forward pass</p></li>
<li><p><strong>bwd_quant</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Callable</span></code>[[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>], <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</span>) – the quantization function specifying how operations should be performed in the backward pass</p></li>
<li><p><strong>keepdim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether the output tensor has <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> retained or not</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the mean operation operation on <cite>x</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qlayernorm">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qlayernorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">QLayerNormFormats(default_fwd,</span> <span class="pre">default_bwd)</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qlayernorm"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qlayernorm" title="Link to this definition">#</a></dt>
<dd><p>Implements the operation as described in the paper <em>Layer Normalization</em> (<a class="reference external" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>),
giving the user control over how the arithmetic is performed during the forward and backward passes through
the <code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter.</p>
<div class="math notranslate nohighlight">
\[y = \frac{x-\mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}}\gamma + \beta\]</div>
<p>The mean and standard deviation are computed over the last <cite>D</cite> dimensions, where <cite>D</cite> is the dimension
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>. For instance, if <code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code> is <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">5)</span></code> (ad 2-D shape),
the mean and standard deviation are computed over the last 2 dimension of the input (i.e., <code class="docutils literal notranslate"><span class="pre">x.mean((-2,</span> <span class="pre">-1))</span></code>).
In a training context, <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are learnable affine transform paremeters of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">normalized_shape</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each
entire channel/plane with the <code class="xref py py-attr docutils literal notranslate"><span class="pre">affine</span></code> option, Layer Normalization applies per-element scale and
bias with <code class="xref py py-attr docutils literal notranslate"><span class="pre">elementwise_affine</span></code>.</p>
</div>
<p>It uses statistics computed from input data in both training and evaluation modes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>normalized_shape</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>] | <code class="xref py py-class docutils literal notranslate"><span class="pre">Size</span></code></span>) – <p>input shape from an expected input of size</p>
<div class="math notranslate nohighlight">
\[[* \times \text{n_shape}[0] \times \text{n_shape}[1]
    \times \ldots \times \text{n_shape}[-1]]\]</div>
<p>If a single integer is used, it is treated as a singleton list, and this function will
normalize over the last dimension which is expected to be of that specific size</p>
</p></li>
<li><p><strong>weight</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the learnable weights <span class="math notranslate nohighlight">\(\gamma\)</span> of shape <span class="math notranslate nohighlight">\(\text{n_shape}\)</span></p></li>
<li><p><strong>bias</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the learnable bias <span class="math notranslate nohighlight">\(\beta\)</span> of shape <span class="math notranslate nohighlight">\(\text{n_shape}\)</span></p></li>
<li><p><strong>eps</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code></span>) – a small value added to the denominator for numerical stability. Default: 1e-5</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QLayerNormFormats" title="mptorch.quant.quant_format.QLayerNormFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QLayerNormFormats</span></code></a></span>) – configuration class for number formats and quantizers to use during
forward and backward computations in layer normalization.</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>the quantized result of the mean operation operation on <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>. Has the same shape as <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qsoftmax">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qsoftmax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qsoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qsoftmax" title="Link to this definition">#</a></dt>
<dd><p>Applies the Softmax function to an n-dimensional input tensor <code class="xref py py-attr docutils literal notranslate"><span class="pre">x</span></code>. Through the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">formats</span></code> parameter it allows one to specify if I/O signals and internal mathematical
operations should be quantized during forward and backward compute chains.</p>
<p>Rescales the elements in the input tensor so that they lie in the range <span class="math notranslate nohighlight">\([0, 1]\)</span> and sum to <span class="math notranslate nohighlight">\(1\)</span>.
It is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{Softmax}(x_i) = \frac{\exp(x_i)}{\sum_j\exp(x_j)}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>dim</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – a dimension along which Softmax will be computed (so every slice along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>
will sum to 1)</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QSoftmaxFormats" title="mptorch.quant.quant_format.QSoftmaxFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QSoftmaxFormats</span></code></a></span>) – configuration class for number formats and quantizers to use during
forward and backward computations in Softmax</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a <code class="xref py py-attr docutils literal notranslate"><span class="pre">Tensor</span></code> of the same dimension and shape as the input with values in the range <span class="math notranslate nohighlight">\([0, 1]\)</span></p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="mptorch.quant.functional.qgelu">
<span class="sig-prename descclassname"><span class="pre">mptorch.quant.functional.</span></span><span class="sig-name descname"><span class="pre">qgelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">formats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approximate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'none'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/functional.html#qgelu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.functional.qgelu" title="Link to this definition">#</a></dt>
<dd><p>Applies the Gaussian Error Linear Units function to the input <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[\text{GELU}(x) = x * \Phi(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi(x)\)</span> is the Cumulative Distribution Function for Gaussian Distribution.</p>
<p>When the approximate argument is <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code>, GELU is estimated with:</p>
<div class="math notranslate nohighlight">
\[\text{GELU}(x) = 0.5 * x * (1 + \tanh(\sqrt{2 / \pi} * (x + 0.044715 * x^3)))\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span>) – the input tensor</p></li>
<li><p><strong>formats</strong> (<span class="sphinx_autodoc_typehints-type"><a class="reference internal" href="#mptorch.quant.quant_format.QGELUFormats" title="mptorch.quant.quant_format.QGELUFormats"><code class="xref py py-class docutils literal notranslate"><span class="pre">QGELUFormats</span></code></a></span>) – configuration class for number formats and quantizers to use during
forward and backward computations in GELU</p></li>
<li><p><strong>approximate</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-data docutils literal notranslate"><span class="pre">Literal</span></code>[<code class="docutils literal notranslate"><span class="pre">'tanh'</span></code>, <code class="docutils literal notranslate"><span class="pre">'none'</span></code>]</span>) – the GELU approximation algorithm to use:
<code class="docutils literal notranslate"><span class="pre">'none'</span></code> | <code class="docutils literal notranslate"><span class="pre">'tanh'</span></code>. Default: <code class="docutils literal notranslate"><span class="pre">'none'</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></span></p>
</dd>
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a <code class="xref py py-attr docutils literal notranslate"><span class="pre">Tensor</span></code> of the same dimension and shape as the input where the GELU function is applied element-wise</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mptorch.quant</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">qt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.testing</span><span class="w"> </span><span class="kn">import</span> <span class="n">assert_close</span>

<span class="n">quant_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">qt</span><span class="o">.</span><span class="n">float_quantize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">man</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">exp</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">rounding</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="n">formats</span> <span class="o">=</span> <span class="n">qt</span><span class="o">.</span><span class="n">QGELUFormats</span><span class="p">(</span>
    <span class="n">input_quant</span><span class="o">=</span><span class="n">quant_func</span><span class="p">,</span>
    <span class="n">output_quant</span><span class="o">=</span><span class="n">quant_func</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">qy</span> <span class="o">=</span> <span class="n">qt</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">qgelu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">formats</span><span class="o">=</span><span class="n">formats</span><span class="p">)</span>
<span class="n">assert_close</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">qy</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="module-mptorch.quant.cublas">
<span id="cublas-acceleration"></span><h2>cuBLAS Acceleration<a class="headerlink" href="#module-mptorch.quant.cublas" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="mptorch.quant.cublas.cublas_acceleration">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mptorch.quant.cublas.</span></span><span class="sig-name descname"><span class="pre">cublas_acceleration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">enabled</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/cublas.html#cublas_acceleration"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.cublas.cublas_acceleration" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>cuBLAS acceleration management.</p>
<p>This class allows enabling and disabling of automatic cuBLAS acceleration
for compatible types. When enabled, all calls for float quantized (batched) GEMMs
(<cite>float_mm</cite> and <cite>float_bmm</cite>) used internally linear and convolutional layers will
use the cuBLAS GEMM functions if the floating point computation formats matches one
of the formats combinations supported by cuBLAS, i.e:
- nearest rounding even mode
- fused-multiply-add enabled
- subnormals enabled
- saturation disabled
- same multiplication/accumulator types matching one of the
[supported combinations](<a class="reference external" href="https://docs.nvidia.com/cuda/cublas/#cublasgemmex">https://docs.nvidia.com/cuda/cublas/#cublasgemmex</a>)</p>
<p>This feature is disabled by default.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>enabled</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether to enable automatic cuBLAS acceleration.</p></li>
<li><p><strong>fast_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – allow internal downcast to lower-precision for tensor
cores. Currently supported are <cite>f16</cite>, <cite>bf16</cite> and <cite>tf32</cite></p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p>cuBLAS acceleration can be enabled/disabled globally using the static
<cite>enable</cite> method; or locally as a context manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mac_format</span> <span class="o">=</span> <span class="n">FloatingPoint</span><span class="p">(</span>
    <span class="n">exp</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">man</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">subnormals</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">saturate</span><span class="o">=</span><span class="kc">False</span> <span class="c1"># F16, supported by cublas</span>
<span class="p">)</span>
<span class="n">layer_formats</span> <span class="o">=</span> <span class="n">QAffineFormats</span><span class="p">(</span>
    <span class="n">fwd_mac</span><span class="o">=</span><span class="p">(</span><span class="n">mac_format</span><span class="p">,),</span>
    <span class="n">bwd_mac</span><span class="o">=</span><span class="p">(</span><span class="n">mac_format</span><span class="p">,),</span>
    <span class="n">fwd_rnd</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">,</span>
    <span class="n">bwd_rnd</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">)</span>
<span class="n">layer</span> <span class="o">=</span> <span class="n">QLinear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">formats</span><span class="o">=</span><span class="n">layer_formats</span><span class="p">)</span>
<span class="k">with</span> <span class="n">cublas_acceleration</span><span class="p">(</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.cublas.cublas_acceleration.enabled">
<span class="sig-name descname"><span class="pre">enabled</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#mptorch.quant.cublas.cublas_acceleration.enabled" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="mptorch.quant.cublas.cublas_acceleration.fast_mode">
<span class="sig-name descname"><span class="pre">fast_mode</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#mptorch.quant.cublas.cublas_acceleration.fast_mode" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mptorch.quant.cublas.cublas_acceleration.enable">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">enable</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">status</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fast_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/mptorch/quant/cublas.html#cublas_acceleration.enable"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#mptorch.quant.cublas.cublas_acceleration.enable" title="Link to this definition">#</a></dt>
<dd><p>Globally enables or disables cuBLAS acceleration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>status</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">bool</span></code></span>) – whether to enable or disable cuBLAS acceleration</p></li>
<li><p><strong>fast_mode</strong> (<span class="sphinx_autodoc_typehints-type"><code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code> | <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code></span>) – use down-conversion to <cite>f16</cite>, <cite>bf16</cite> or <cite>tf32</cite> for faster GEMM when possible</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="examples.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Examples</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-mptorch.number">Number formats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.Number"><code class="docutils literal notranslate"><span class="pre">Number</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.FloatType"><code class="docutils literal notranslate"><span class="pre">FloatType</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.FixedPoint"><code class="docutils literal notranslate"><span class="pre">FixedPoint</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.FloatingPoint"><code class="docutils literal notranslate"><span class="pre">FloatingPoint</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.FloatingPoint.is_fp32"><code class="docutils literal notranslate"><span class="pre">FloatingPoint.is_fp32</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.FloatingPoint.is_fp16"><code class="docutils literal notranslate"><span class="pre">FloatingPoint.is_fp16</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.FloatingPoint.is_bfloat16"><code class="docutils literal notranslate"><span class="pre">FloatingPoint.is_bfloat16</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.BlockFloatingPoint"><code class="docutils literal notranslate"><span class="pre">BlockFloatingPoint</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.SuperNormalFloat"><code class="docutils literal notranslate"><span class="pre">SuperNormalFloat</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.number.Binary8"><code class="docutils literal notranslate"><span class="pre">Binary8</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-mptorch.quant">Quantization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.fixed_point_quantize"><code class="docutils literal notranslate"><span class="pre">fixed_point_quantize()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.block_quantize"><code class="docutils literal notranslate"><span class="pre">block_quantize()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.float_quantize"><code class="docutils literal notranslate"><span class="pre">float_quantize()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.binary8_quantize"><code class="docutils literal notranslate"><span class="pre">binary8_quantize()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.superfp_quantize"><code class="docutils literal notranslate"><span class="pre">superfp_quantize()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.quantizer"><code class="docutils literal notranslate"><span class="pre">quantizer()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.qlinear_mp"><code class="docutils literal notranslate"><span class="pre">qlinear_mp</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.qlinear_mp.backward"><code class="docutils literal notranslate"><span class="pre">qlinear_mp.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.qlinear_mp.forward"><code class="docutils literal notranslate"><span class="pre">qlinear_mp.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-mptorch.quant.quant_format">Quantization Formats</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.quant_format.QAffineFormats"><code class="docutils literal notranslate"><span class="pre">QAffineFormats</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.quant_format.QSoftmaxFormats"><code class="docutils literal notranslate"><span class="pre">QSoftmaxFormats</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.quant_format.QLayerNormFormats"><code class="docutils literal notranslate"><span class="pre">QLayerNormFormats</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.quant_format.QGELUFormats"><code class="docutils literal notranslate"><span class="pre">QGELUFormats</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-mptorch.quant.modules">Modules</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.Quantizer"><code class="docutils literal notranslate"><span class="pre">Quantizer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.Quantizer.forward"><code class="docutils literal notranslate"><span class="pre">Quantizer.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear"><code class="docutils literal notranslate"><span class="pre">QLinear</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear.weight"><code class="docutils literal notranslate"><span class="pre">QLinear.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear.bias"><code class="docutils literal notranslate"><span class="pre">QLinear.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear.forward"><code class="docutils literal notranslate"><span class="pre">QLinear.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear.quant_function"><code class="docutils literal notranslate"><span class="pre">QLinear.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QLinear.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinear.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QLinear.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLazyLinear"><code class="docutils literal notranslate"><span class="pre">QLazyLinear</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLazyLinear.cls_to_become"><code class="docutils literal notranslate"><span class="pre">QLazyLinear.cls_to_become</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLazyLinear.initialize_parameters"><code class="docutils literal notranslate"><span class="pre">QLazyLinear.initialize_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLazyLinear.reset_parameters"><code class="docutils literal notranslate"><span class="pre">QLazyLinear.reset_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLazyLinear.weight"><code class="docutils literal notranslate"><span class="pre">QLazyLinear.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLazyLinear.bias"><code class="docutils literal notranslate"><span class="pre">QLazyLinear.bias</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinearMP"><code class="docutils literal notranslate"><span class="pre">QLinearMP</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinearMP.forward"><code class="docutils literal notranslate"><span class="pre">QLinearMP.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinearMP.quant_function"><code class="docutils literal notranslate"><span class="pre">QLinearMP.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinearMP.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QLinearMP.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLinearMP.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QLinearMP.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d"><code class="docutils literal notranslate"><span class="pre">QConv1d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d.weight"><code class="docutils literal notranslate"><span class="pre">QConv1d.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d.bias"><code class="docutils literal notranslate"><span class="pre">QConv1d.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d.forward"><code class="docutils literal notranslate"><span class="pre">QConv1d.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d.quant_function"><code class="docutils literal notranslate"><span class="pre">QConv1d.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QConv1d.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv1d.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QConv1d.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d"><code class="docutils literal notranslate"><span class="pre">QConv2d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d.weight"><code class="docutils literal notranslate"><span class="pre">QConv2d.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d.bias"><code class="docutils literal notranslate"><span class="pre">QConv2d.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d.forward"><code class="docutils literal notranslate"><span class="pre">QConv2d.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d.quant_function"><code class="docutils literal notranslate"><span class="pre">QConv2d.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QConv2d.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv2d.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QConv2d.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d"><code class="docutils literal notranslate"><span class="pre">QConv3d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d.weight"><code class="docutils literal notranslate"><span class="pre">QConv3d.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d.bias"><code class="docutils literal notranslate"><span class="pre">QConv3d.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d.forward"><code class="docutils literal notranslate"><span class="pre">QConv3d.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d.quant_function"><code class="docutils literal notranslate"><span class="pre">QConv3d.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QConv3d.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConv3d.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QConv3d.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d.weight"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d.bias"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d.forward"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d.quant_function"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose1d.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QConvTranspose1d.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d.weight"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d.bias"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d.forward"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d.quant_function"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose2d.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QConvTranspose2d.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d.weight"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d.bias"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d.forward"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d.quant_function"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QConvTranspose3d.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QConvTranspose3d.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QAvgPool2d"><code class="docutils literal notranslate"><span class="pre">QAvgPool2d</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QAvgPool2d.forward"><code class="docutils literal notranslate"><span class="pre">QAvgPool2d.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QBatchNorm1d"><code class="docutils literal notranslate"><span class="pre">QBatchNorm1d</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QBatchNorm2d"><code class="docutils literal notranslate"><span class="pre">QBatchNorm2d</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm"><code class="docutils literal notranslate"><span class="pre">QLayerNorm</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm.weight"><code class="docutils literal notranslate"><span class="pre">QLayerNorm.weight</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm.bias"><code class="docutils literal notranslate"><span class="pre">QLayerNorm.bias</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm.forward"><code class="docutils literal notranslate"><span class="pre">QLayerNorm.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm.quant_function"><code class="docutils literal notranslate"><span class="pre">QLayerNorm.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm.quant_parameters"><code class="docutils literal notranslate"><span class="pre">QLayerNorm.quant_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QLayerNorm.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QLayerNorm.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QSoftmax"><code class="docutils literal notranslate"><span class="pre">QSoftmax</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">QSoftmax.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QSoftmax.quant_function"><code class="docutils literal notranslate"><span class="pre">QSoftmax.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QSoftmax.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QSoftmax.reset_quant_function()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QGELU"><code class="docutils literal notranslate"><span class="pre">QGELU</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QGELU.forward"><code class="docutils literal notranslate"><span class="pre">QGELU.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QGELU.quant_function"><code class="docutils literal notranslate"><span class="pre">QGELU.quant_function()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.modules.QGELU.reset_quant_function"><code class="docutils literal notranslate"><span class="pre">QGELU.reset_quant_function()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-mptorch.quant.functional">Functional</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qlinear"><code class="docutils literal notranslate"><span class="pre">qlinear()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qlinear_mp"><code class="docutils literal notranslate"><span class="pre">qlinear_mp</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qlinear_mp.forward"><code class="docutils literal notranslate"><span class="pre">qlinear_mp.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qlinear_mp.backward"><code class="docutils literal notranslate"><span class="pre">qlinear_mp.backward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qmatmul"><code class="docutils literal notranslate"><span class="pre">qmatmul()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qmm"><code class="docutils literal notranslate"><span class="pre">qmm()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qadd"><code class="docutils literal notranslate"><span class="pre">qadd()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qmul"><code class="docutils literal notranslate"><span class="pre">qmul()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qsqrt"><code class="docutils literal notranslate"><span class="pre">qsqrt()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qdiv"><code class="docutils literal notranslate"><span class="pre">qdiv()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qpow"><code class="docutils literal notranslate"><span class="pre">qpow()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qsum"><code class="docutils literal notranslate"><span class="pre">qsum()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qmean"><code class="docutils literal notranslate"><span class="pre">qmean()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qlayernorm"><code class="docutils literal notranslate"><span class="pre">qlayernorm()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qsoftmax"><code class="docutils literal notranslate"><span class="pre">qsoftmax()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.functional.qgelu"><code class="docutils literal notranslate"><span class="pre">qgelu()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-mptorch.quant.cublas">cuBLAS Acceleration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.cublas.cublas_acceleration"><code class="docutils literal notranslate"><span class="pre">cublas_acceleration</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.cublas.cublas_acceleration.enabled"><code class="docutils literal notranslate"><span class="pre">cublas_acceleration.enabled</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.cublas.cublas_acceleration.fast_mode"><code class="docutils literal notranslate"><span class="pre">cublas_acceleration.fast_mode</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mptorch.quant.cublas.cublas_acceleration.enable"><code class="docutils literal notranslate"><span class="pre">cublas_acceleration.enable()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/api.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-2025, The MPTorch developers.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>