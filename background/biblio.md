## Floating-Point
- [Wikipedia article](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
- [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://pages.cs.wisc.edu/~david/courses/cs552/S12/handouts/goldberg-floating-point.pdf)
- [Numerical Computing with IEEE Floating Point Arithmetic](https://cosweb1.fau.edu/~jmirelesjames/ODE_course/Numerical_Computing_with_IEEE_Floating_Point_Arithmetic.pdf) by Michael Overton, first six chapters are a good tutorial-style introduction to floating-point numbers

## Transformers and LLMs
- [Andrej Karpathy's lectures: NN from zero to hero](https://github.com/karpathy/nn-zero-to-hero): a nice and pedagogical introduction to PyTorch, DNN training and LLMs (you should definitely watch these and play with the code!)
- [Andrej Karpathy's llm.c project](https://github.com/karpathy/llm.c): LLM (GPT-2) training code in pure C/CUDA. This is an awesome project for seeing how to train an LLM from scratch and can serve as a starting point for implementing some of the functionalities that we want to include in mptorch over the summer.
- [ViT's for image classification on small datasets](https://github.com/s-chh/PyTorch-Vision-Transformer-ViT-MNIST-CIFAR10)

## CUDA